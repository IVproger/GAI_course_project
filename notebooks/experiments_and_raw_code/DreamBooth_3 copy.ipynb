{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from PIL import Image\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 1. Load & Prepare Dataset with DreamBooth Prompts\n",
    "# -------------------------------\n",
    "# Set DreamBooth prompt parameters.\n",
    "use_dreambooth_prompts = True\n",
    "subject_identifier = \"sks\"        # Unique token (e.g., a rare token)\n",
    "subject_class = \"dog\"             # The coarse class (e.g., \"dog\")\n",
    "subject_prompt = f\"a {subject_identifier} {subject_class}\"\n",
    "class_prompt = f\"a {subject_class}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset (using your chosen subset)\n",
    "dataset = load_dataset('poloclub/diffusiondb', 'large_random_1k', trust_remote_code=True)\n",
    "full_dataset = concatenate_datasets([split for split in dataset.values()])\n",
    "\n",
    "# Replace prompt with DreamBooth subject prompt if desired\n",
    "if use_dreambooth_prompts:\n",
    "    filtered_dataset = full_dataset.map(lambda x: {'image': x['image'], 'prompt': subject_prompt})\n",
    "else:\n",
    "    filtered_dataset = full_dataset.map(lambda x: {'image': x['image'], 'prompt': x['prompt']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first image and its prompt for verification\n",
    "first_item = filtered_dataset[0]\n",
    "plt.imshow(first_item['image'])\n",
    "plt.axis('off')  \n",
    "plt.show()\n",
    "print(\"Subject Prompt:\", first_item['prompt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 2. Define Tokenizer, Text Encoder and Image Transforms\n",
    "# -------------------------------\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "text_encoder = CLIPTextModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"text_encoder\")\n",
    "# Define image transformations\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, tokenizer, transforms):\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        image = item['image'].convert(\"RGB\")\n",
    "        prompt = item['prompt']\n",
    "        image = self.transforms(image)\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\", max_length=self.tokenizer.model_max_length, \n",
    "                                padding=\"max_length\", truncation=True)\n",
    "        input_ids = inputs[\"input_ids\"].squeeze()\n",
    "        attention_mask = inputs[\"attention_mask\"].squeeze()\n",
    "        return image, input_ids, attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_dataset = CustomDataset(filtered_dataset, tokenizer, train_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(examples):\n",
    "    pixel_values = torch.stack([example[0] for example in examples])\n",
    "    pixel_values = pixel_values.to(memory_format=torch.contiguous_format).float()\n",
    "    input_ids = torch.stack([example[1] for example in examples])\n",
    "    attention_mask = torch.stack([example[2] for example in examples])\n",
    "    return {\"pixel_values\": pixel_values, \"input_ids\": input_ids, \"attention_mask\": attention_mask}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size = 10\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    custom_dataset,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    batch_size=train_batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 3. Load Pretrained Diffusion Components\n",
    "# -------------------------------\n",
    "from diffusers import AutoencoderKL, DDPMScheduler, UNet2DConditionModel, StableDiffusionPipeline\n",
    "from diffusers.optimization import get_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model_name_or_path = \"CompVis/stable-diffusion-v1-4\"\n",
    "revision = None\n",
    "variant = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_scheduler = DDPMScheduler.from_pretrained(pretrained_model_name_or_path, subfolder=\"scheduler\")\n",
    "vae = AutoencoderKL.from_pretrained(pretrained_model_name_or_path, subfolder=\"vae\", revision=revision, variant=variant)\n",
    "unet = UNet2DConditionModel.from_pretrained(pretrained_model_name_or_path, subfolder=\"unet\", revision=revision, variant=variant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze VAE, text encoder and (base) unet parameters for memory and stability.\n",
    "mixed_precision = \"fp16\"\n",
    "unet.requires_grad_(False)\n",
    "vae.requires_grad_(False)\n",
    "text_encoder.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set weight dtype and move models to device.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "weight_dtype = torch.float16 if mixed_precision == \"fp16\" else torch.float32\n",
    "unet.to(device, dtype=weight_dtype)\n",
    "vae.to(device, dtype=weight_dtype)\n",
    "text_encoder.to(device, dtype=weight_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 4. Set Up LoRA Adapter (Optional)\n",
    "# -------------------------------\n",
    "from peft import LoraConfig\n",
    "from diffusers.training_utils import cast_training_params\n",
    "\n",
    "unet_lora_config = LoraConfig(\n",
    "    r=40,\n",
    "    lora_alpha=16,\n",
    "    init_lora_weights=\"gaussian\",\n",
    "    target_modules=[\"to_k\", \"to_q\", \"to_v\", \"to_out.0\"],\n",
    ")\n",
    "unet.add_adapter(unet_lora_config)\n",
    "if mixed_precision == \"fp16\":\n",
    "    cast_training_params(unet, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 5. Set Up Prior Preservation Mechanism\n",
    "# -------------------------------\n",
    "use_prior_preservation = True\n",
    "prior_loss_weight = 1.0  # lambda in the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_prior_preservation:\n",
    "    # Create a frozen copy of the original UNet for generating prior samples.\n",
    "    unet_pretrained = UNet2DConditionModel.from_pretrained(pretrained_model_name_or_path, \n",
    "                                                           subfolder=\"unet\", revision=revision, variant=variant)\n",
    "    unet_pretrained.to(device, dtype=weight_dtype)\n",
    "    unet_pretrained.eval()\n",
    "    \n",
    "    # Build a separate pipeline using the frozen UNet.\n",
    "    pipeline_pretrained = StableDiffusionPipeline(\n",
    "        vae=vae,\n",
    "        text_encoder=text_encoder,\n",
    "        tokenizer=tokenizer,\n",
    "        unet=unet_pretrained,\n",
    "        scheduler=noise_scheduler,\n",
    "        safety_checker=None,\n",
    "        feature_extractor=None,\n",
    "    ).to(device)\n",
    "    \n",
    "    # Pre-generate a small set of prior samples.\n",
    "    num_prior_images = 50  # For demo; in practice, many more samples are used.\n",
    "    prior_latents_list = []\n",
    "    print(\"Generating prior preservation latents...\")\n",
    "    for i in range(num_prior_images):\n",
    "        with torch.autocast(\"cuda\"):\n",
    "            output = pipeline_pretrained(class_prompt, num_inference_steps=50, guidance_scale=7.5)\n",
    "        gen_image = output.images[0].convert(\"RGB\")\n",
    "        # Convert generated image to tensor.\n",
    "        gen_image_tensor = train_transforms(gen_image).unsqueeze(0).to(device, dtype=weight_dtype)\n",
    "        with torch.no_grad():\n",
    "            latent = vae.encode(gen_image_tensor).latent_dist.sample() * vae.config.scaling_factor\n",
    "        prior_latents_list.append(latent)\n",
    "    \n",
    "    # Prepare prior prompt embeddings (to be reused each iteration)\n",
    "    prior_inputs = tokenizer(class_prompt, return_tensors=\"pt\", max_length=tokenizer.model_max_length,\n",
    "                             padding=\"max_length\", truncation=True)\n",
    "    prior_input_ids = prior_inputs[\"input_ids\"].to(device)\n",
    "    prior_attention_mask = prior_inputs[\"attention_mask\"].to(device)\n",
    "    with torch.no_grad():\n",
    "        prior_embeddings = text_encoder(input_ids=prior_input_ids, attention_mask=prior_attention_mask).last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 6. Set Up Training Hyperparameters & Optimizer\n",
    "# -------------------------------\n",
    "num_train_epochs = 3\n",
    "learning_rate = 1e-4\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    filter(lambda p: p.requires_grad, unet.parameters()),\n",
    "    lr=learning_rate,\n",
    "    betas=(0.9, 0.999),\n",
    "    weight_decay=1e-2,\n",
    "    eps=1e-08,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use total steps based on dataset size (using custom_dataset length)\n",
    "num_training_steps = num_train_epochs * len(custom_dataset) // train_batch_size\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"constant\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=500,\n",
    "    num_training_steps=num_training_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 7. Training Loop with Dual (Subject + Prior) Losses\n",
    "# -------------------------------\n",
    "for epoch in tqdm(range(num_train_epochs), desc=\"Epochs\"):\n",
    "    unet.train()\n",
    "    epoch_losses = []\n",
    "\n",
    "    for step, batch in enumerate(tqdm(train_dataloader, desc=\"Batches\", leave=False)):\n",
    "        # Move subject batch to device.\n",
    "        pixel_values = batch[\"pixel_values\"].to(dtype=weight_dtype, device=device)\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "\n",
    "        # Convert subject images to latent space.\n",
    "        latents = vae.encode(pixel_values).latent_dist.sample() * vae.config.scaling_factor\n",
    "\n",
    "        # Sample noise and timesteps for subject images.\n",
    "        noise = torch.randn_like(latents, device=device)\n",
    "        timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, \n",
    "                                  (latents.shape[0],), device=device).long()\n",
    "        noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
    "\n",
    "        # Get text embeddings from subject prompts.\n",
    "        outputs = text_encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        embeddings = outputs.last_hidden_state\n",
    "\n",
    "        # Predict noise residual for subject branch.\n",
    "        model_pred = unet(noisy_latents, timesteps, encoder_hidden_states=embeddings,\n",
    "                          encoder_attention_mask=attention_mask, return_dict=False)[0]\n",
    "        loss_subject = F.mse_loss(model_pred.float(), noise.float(), reduction=\"mean\")\n",
    "\n",
    "        # ----- Prior Preservation Loss -----\n",
    "        if use_prior_preservation:\n",
    "            # Sample a batch of prior latents randomly from the pre-generated list.\n",
    "            indices = torch.randint(0, len(prior_latents_list), (latents.shape[0],))\n",
    "            prior_batch = torch.cat([prior_latents_list[i] for i in indices], dim=0)  # shape: (B, C, H, W)\n",
    "\n",
    "            # For the prior branch, sample noise and timesteps.\n",
    "            noise_prior = torch.randn_like(prior_batch, device=device)\n",
    "            t_prior = torch.randint(0, noise_scheduler.config.num_train_timesteps, \n",
    "                                      (prior_batch.shape[0],), device=device).long()\n",
    "            noisy_prior_latents = noise_scheduler.add_noise(prior_batch, noise_prior, t_prior)\n",
    "\n",
    "            # Expand prior embeddings to match batch size.\n",
    "            prior_embeddings_expanded = prior_embeddings.expand(noisy_prior_latents.shape[0], -1, -1)\n",
    "            prior_attention_mask_expanded = prior_attention_mask.expand(noisy_prior_latents.shape[0], -1)\n",
    "\n",
    "            # Predict noise residual for prior branch.\n",
    "            model_pred_prior = unet(noisy_prior_latents, t_prior, encoder_hidden_states=prior_embeddings_expanded,\n",
    "                                    encoder_attention_mask=prior_attention_mask_expanded, return_dict=False)[0]\n",
    "            loss_prior = F.mse_loss(model_pred_prior.float(), noise_prior.float(), reduction=\"mean\")\n",
    "        else:\n",
    "            loss_prior = 0.0\n",
    "\n",
    "        # Total loss is the sum of subject loss and weighted prior loss.\n",
    "        loss = loss_subject + prior_loss_weight * loss_prior\n",
    "\n",
    "        optimizer.zero_grad()  \n",
    "        loss.backward()  \n",
    "        optimizer.step()  \n",
    "        lr_scheduler.step() \n",
    "\n",
    "        epoch_losses.append(loss.item())\n",
    "\n",
    "    avg_loss = sum(epoch_losses) / len(epoch_losses)\n",
    "    print(f\"Epoch {epoch + 1}/{num_train_epochs}, Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 8. Inference: Generate Images using the Fine-Tuned Model\n",
    "# -------------------------------\n",
    "from diffusers import StableDiffusionPipeline\n",
    "\n",
    "pipeline = StableDiffusionPipeline(\n",
    "    vae=vae,\n",
    "    text_encoder=text_encoder,\n",
    "    tokenizer=tokenizer,\n",
    "    unet=unet,\n",
    "    scheduler=noise_scheduler,\n",
    "    safety_checker=None,\n",
    "    feature_extractor=None,\n",
    ")\n",
    "pipeline = pipeline.to(device)\n",
    "\n",
    "# Use a prompt with the unique subject identifier to generate subject images.\n",
    "prompt = subject_prompt + \" in a beautiful landscape\"\n",
    "with torch.autocast(device.type):\n",
    "    images = pipeline(prompt, num_inference_steps=100, guidance_scale=7.5).images\n",
    "\n",
    "for idx, img in enumerate(images):\n",
    "    plt.imshow(img)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    img.save(f\"generated_image_{idx}.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

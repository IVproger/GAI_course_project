{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from diffusers import StableDiffusionPipeline, LMSDiscreteScheduler\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Configuration -----\n",
    "subject_images_path = \"./subject_images\"  # Folder with subject images\n",
    "subject_prompt = \"a unique_dog\"  # e.g. unique identifier + class noun\n",
    "class_prompt = \"a dog\"         # Class prompt for prior preservation\n",
    "num_train_steps = 1000\n",
    "batch_size = 1\n",
    "learning_rate = 5e-6          # Example: 5e-6 for Stable Diffusion\n",
    "lambda_prior = 1.0            # Weight for the class prior loss\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Set Up Data Transforms and Dataloader -----\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((512, 512)),\n",
    "    transforms.CenterCrop(512),\n",
    "    transforms.ToTensor(),\n",
    "    # Normalize images to [-1, 1] (the VAE expects inputs in this range)\n",
    "    transforms.Normalize([0.5], [0.5]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use ImageFolder assuming your images are organized under a class folder.\n",
    "subject_dataset = datasets.ImageFolder(subject_images_path, transform=transform)\n",
    "subject_dataloader = DataLoader(subject_dataset, batch_size=batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Load Pretrained Models and Tokenizer -----\n",
    "# Here we use the Stable Diffusion v1-5 model from Hugging Face.\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-v1-5\",\n",
    "    revision=\"fp16\",\n",
    "    torch_dtype=torch.float16,\n",
    ").to(device)\n",
    "pipe.scheduler = LMSDiscreteScheduler.from_config(pipe.scheduler.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract components\n",
    "unet = pipe.unet  # The diffusion network\n",
    "vae = pipe.vae    # The variational autoencoder\n",
    "text_encoder = pipe.text_encoder\n",
    "tokenizer = pipe.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set train mode for modules we want to fine-tune.\n",
    "unet.train()\n",
    "text_encoder.train()\n",
    "# Optionally, you might freeze the VAE if you do not wish to update it.\n",
    "vae.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Training Setup -----\n",
    "optimizer = optim.AdamW(list(unet.parameters()) + list(text_encoder.parameters()), lr=learning_rate)\n",
    "scheduler = pipe.scheduler  # Using the pipelineâ€™s scheduler for noise schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Helper Function: Generate Class Prior Sample -----\n",
    "def generate_class_prior_sample(prompt, num_inference_steps=50):\n",
    "    \"\"\"\n",
    "    Use the frozen pipeline to generate a class prior image.\n",
    "    In practice, you may want to precompute a bank of these samples.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        output = pipe(prompt, num_inference_steps=num_inference_steps)\n",
    "    image = output.images[0]\n",
    "    # Convert PIL image to tensor with same preprocessing as subject images.\n",
    "    image = transform(image).unsqueeze(0).to(device)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Infinite iterator for the dataloader\n",
    "def cycle(loader):\n",
    "    while True:\n",
    "        for data in loader:\n",
    "            yield data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_iter = cycle(subject_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Training Loop -----\n",
    "for step in range(num_train_steps):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # --- Subject Image and Loss ---\n",
    "    # Get one batch of subject images.\n",
    "    (subject_images, _), = [next(subject_iter)]  # subject_images: [B, C, H, W]\n",
    "    subject_images = subject_images.to(device)\n",
    "    \n",
    "    # Encode subject images to latent space using the VAE\n",
    "    with torch.no_grad():\n",
    "        latent_dist = vae.encode(subject_images).latent_dist\n",
    "        latents = latent_dist.sample() * 0.18215  # scaling factor\n",
    "    \n",
    "    # Tokenize the subject prompt\n",
    "    subject_tokens = tokenizer(subject_prompt, padding=\"max_length\", truncation=True, max_length=tokenizer.model_max_length, return_tensors=\"pt\").input_ids.to(device)\n",
    "    with torch.no_grad():\n",
    "        subject_text_embeddings = text_encoder(subject_tokens)[0]\n",
    "    \n",
    "    # Sample random noise and timesteps\n",
    "    t = torch.randint(0, scheduler.num_train_timesteps, (batch_size,), device=device).long()\n",
    "    noise = torch.randn_like(latents)\n",
    "    noisy_latents = scheduler.add_noise(latents, noise, t)\n",
    "    \n",
    "    # Predict noise using UNet\n",
    "    model_pred = unet(noisy_latents, t, encoder_hidden_states=subject_text_embeddings).sample\n",
    "    loss_subject = ((model_pred - noise) ** 2).mean()\n",
    "    \n",
    "    # --- Class Prior Loss ---\n",
    "    # Generate a class prior image using the frozen pipeline.\n",
    "    class_image = generate_class_prior_sample(class_prompt)\n",
    "    with torch.no_grad():\n",
    "        latent_dist_class = vae.encode(class_image).latent_dist\n",
    "        latents_class = latent_dist_class.sample() * 0.18215\n",
    "    \n",
    "    # Tokenize the class prompt\n",
    "    class_tokens = tokenizer(class_prompt, padding=\"max_length\", truncation=True, max_length=tokenizer.model_max_length, return_tensors=\"pt\").input_ids.to(device)\n",
    "    with torch.no_grad():\n",
    "        class_text_embeddings = text_encoder(class_tokens)[0]\n",
    "    \n",
    "    # Use a separate random timestep and noise for the class prior sample.\n",
    "    t_class = torch.randint(0, scheduler.num_train_timesteps, (1,), device=device).long()\n",
    "    noise_class = torch.randn_like(latents_class)\n",
    "    noisy_latents_class = scheduler.add_noise(latents_class, noise_class, t_class)\n",
    "    \n",
    "    model_pred_class = unet(noisy_latents_class, t_class, encoder_hidden_states=class_text_embeddings).sample\n",
    "    loss_class = ((model_pred_class - noise_class) ** 2).mean()\n",
    "    \n",
    "    # Combine losses\n",
    "    loss = loss_subject + lambda_prior * loss_class\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if step % 100 == 0:\n",
    "        print(f\"Step {step}: Total Loss = {loss.item():.4f} | Subject Loss = {loss_subject.item():.4f} | Class Loss = {loss_class.item():.4f}\")\n",
    "\n",
    "print(\"Training complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
